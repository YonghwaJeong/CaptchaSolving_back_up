{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/taemmini/2023_Fall_HUFS_ML_Project_7/blob/main/CAPTCHA_Project_CRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import urllib.request\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "def convert_and_blur(input_image_path, output_image_path):\n",
    "    img = Image.open(input_image_path)\n",
    "    new_img = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
    "    new_img.paste(img, (0, 0), img)\n",
    "\n",
    "    blurred_image = new_img.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "    blurred_image.save(output_image_path, \"PNG\")\n",
    "\n",
    "input_image_path = r\"C:\\Users\\yongh\\Desktop\\MLTeamProject\\resource\\Macro_Test\\CaptchaImage.png\"\n",
    "output_image_path = r\"C:\\Users\\yongh\\Desktop\\MLTeamProject\\resource\\Macro_Test\\CaptchaImage_converted.png\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://ticket.melon.com/main/index.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31C0AF49AE92ED12BD2C6CBE4FFC4C17', 'F9F65D69558FF4ADD701C5042AEA27FD']\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "# 수동 로그인 후 자동화\n",
    "\n",
    "# 예매 버튼을 클릭하고, 팝업 창으로 focus를 전환\n",
    "driver.find_element_by_id('ticketReservation_Btn').click()\n",
    "time.sleep(0.3)\n",
    "print(driver.window_handles)\n",
    "handle = driver.window_handles[1]\n",
    "driver.switch_to.window(handle)\n",
    "\n",
    "driver.set_window_position(0, 0)\n",
    "\n",
    "# 팝업 창에서 CAPTCHA 이미지를 찾아 지정된 경로로 다운로드\n",
    "image = driver.find_element_by_id('captchaImg')\n",
    "url = image.get_attribute('src')\n",
    "urllib.request.urlretrieve(url, r\"C:\\Users\\yongh\\Desktop\\MLTeamProject\\resource\\Macro_Test\\CaptchaImage.png\")\n",
    "\n",
    "# 다운받은 이미지를 RGBA->RGB로 전환하고 가우시안 블러처리\n",
    "time.sleep(0.1)\n",
    "convert_and_blur(input_image_path, output_image_path)\n",
    "\n",
    "# \n",
    "time.sleep(0.1)\n",
    "captcha_answer = get_prediction(prediction_model, output_image_path)\n",
    "captcha_input = driver.find_element_by_class_name('inputType')\n",
    "captcha_input.send_keys(captcha_answer)\n",
    "summit_button = driver.find_element_by_id('btnComplete').click()\n",
    "\n",
    "# back to main\n",
    "main_window = driver.window_handles[0]\n",
    "driver.switch_to.window(main_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to main\n",
    "\n",
    "main_window = driver.window_handles[0]\n",
    "driver.switch_to.window(main_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to pop-up window\n",
    "\n",
    "handle = driver.window_handles[1]\n",
    "driver.switch_to.window(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이하 학습 모델 로딩 관련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yongh\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_with_label(folder_path : str, extension : str):\n",
    "    images = glob(r\".\\resource\\%s\\*.%s\"%(folder_path, extension))\n",
    "    labels = [img.split(os.path.sep)[3].split('.'+extension)[0].split('_')[0] for img in images]\n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "26\n",
      "WARNING:tensorflow:From C:\\Users\\yongh\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yongh\\anaconda3\\Lib\\site-packages\\numpy\\core\\numeric.py:2468: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "images, labels = load_image_with_label('MelonCaptcha_blurred','png')\n",
    "unique_char = sorted(list(set(''.join(labels))))\n",
    "print(unique_char)\n",
    "print(len(unique_char))\n",
    "\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary = list(unique_char), num_oov_indices = 0, mask_token = None\n",
    ")\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary = char_to_num.get_vocabulary(), num_oov_indices = 0, mask_token = None, invert = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 280\n",
    "img_height = 80\n",
    "\n",
    "def encode_image_only(path):\n",
    "  img = tf.io.read_file(path)\n",
    "  img = tf.io.decode_png(img, channels=1)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  img = tf.image.resize(img, [img_height, img_width])\n",
    "  img = tf.transpose(img, perm=[1,0,2])\n",
    "    \n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype='int64')\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype='int64')\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype='int64')\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred\n",
    "\n",
    "def build_model():\n",
    "    input_img = layers.Input(\n",
    "        shape=(img_width, img_height, 1), name='image', dtype='float32'\n",
    "    )\n",
    "    labels = layers.Input(name='label', shape=(None,), dtype='float32')\n",
    "\n",
    "    # Convolution, Maxpooling을 각각 두번하여 차원을 축소함\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        32,\n",
    "        (3, 3),\n",
    "        activation='relu',\n",
    "        kernel_initializer='he_normal',\n",
    "        padding='same',\n",
    "        name='Conv1',\n",
    "    )(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), name='pool1')(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation='relu',\n",
    "        kernel_initializer='he_normal',\n",
    "        padding='same',\n",
    "        name='Conv2',\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name='pool2')(x)\n",
    "\n",
    "    new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
    "    x = layers.Reshape(target_shape=new_shape, name='reshape')(x)\n",
    "    x = layers.Dense(64, activation='relu', name='dense1')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # RNN을 두 차례 사용\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "\n",
    "    # 출력층 구성\n",
    "    x = layers.Dense(\n",
    "        len(char_to_num.get_vocabulary()) + 1, activation='softmax', name='dense2'\n",
    "    )(x)\n",
    "\n",
    "    # CTC 손실함수 적용\n",
    "    output = CTCLayer(name='ctc_loss')(labels, x)\n",
    "\n",
    "    # 모델 구성\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_img, labels], outputs=output, name='ocr_model_v1'\n",
    "    )\n",
    "    # 케라스에서 제공하는 최적화\n",
    "    opt = keras.optimizers.Adam()\n",
    "\n",
    "    model.compile(optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yongh\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yongh\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yongh\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\save.py:538: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2bb07a3cc90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.load_weights('.\\GaussianBlurBest_in30Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = keras.models.Model(\n",
    "  model.get_layer(name='image').input, model.get_layer(name='dense2').output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :max_length]\n",
    "    output_text = []\n",
    "    # encode된 글자를 복원하여 리스트에 저장\n",
    "    for res in results:\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode('utf-8')\n",
    "        output_text.append(res)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, path : str):\n",
    "    image_tensor = encode_image_only(path)\n",
    "    image_tensor = tf.expand_dims(image_tensor,0)\n",
    "    pred = model.predict(image_tensor)\n",
    "    pred_text = decode_batch_predictions(pred)\n",
    "    \n",
    "    return pred_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 820ms/step\n",
      "WARNING:tensorflow:From C:\\Users\\yongh\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "DNEOAN\n"
     ]
    }
   ],
   "source": [
    "print(get_prediction(prediction_model, output_image_path))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPtVv1LESr7zJB2Ue2RX+Y6",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
